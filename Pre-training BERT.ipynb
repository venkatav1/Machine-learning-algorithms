{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bSz5jzj61nHc"
      },
      "source": [
        "# BERT model pre-training from scratch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "-8kZmr4ItGUj"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import re\n",
        "from   random import *\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import spacy\n",
        "\n",
        "torch.set_default_tensor_type('torch.cuda.FloatTensor') # for gpu training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Import small subset of Shakespeare's work"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['first citizen:\\nbefore we proceed any further hear me speak\\n\\n',\n",
              " 'all:\\nspeak speak\\n\\n',\n",
              " 'first citizen:\\nyou are all resolved rather to die than to famish\\n\\n',\n",
              " 'all:\\nresolved resolved\\n\\n',\n",
              " 'first citizen:\\nfirst you know caius marcius is chief enemy to the people\\n\\n',\n",
              " \"all:\\nwe know't we know't\\n\\n\",\n",
              " \"first citizen:\\nlet us kill him and we'll have corn at our own price\\nis't a verdict\\n\\n\",\n",
              " \"all:\\nno more talking on't; let it be done: away away\\n\\nsecond citizen:\\none word good citizens\\n\\n\",\n",
              " 'first citizen:\\nwe are accounted poor citizens the patricians good\\n',\n",
              " 'what authority surfeits on would relieve us: if they\\nwould yield us but the superfluity while it were\\nwholesome we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us the object of our misery is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them let us revenge this with\\nour pikes ere we become rakes: for the gods know i\\nspeak this in hunger for bread not in thirst for revenge']"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "with open (\"input.txt\", \"r\") as f:\n",
        "    raw_text = f.read()\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(raw_text)\n",
        "sentences = list(doc.sents)\n",
        "\n",
        "text = [x.text.lower() for x in sentences] #lower case\n",
        "text = [re.sub(\"[.,!?\\\\-]\", '', x) for x in text] #clean all symbols\n",
        "\n",
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first citizen:\n",
            "before we proceed any further hear me speak\n",
            "\n",
            " _____\n",
            "['first', 'citizen:', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak']\n",
            "all:\n",
            "speak speak\n",
            "\n",
            " _____\n",
            "['all:', 'speak', 'speak']\n",
            "first citizen:\n",
            "you are all resolved rather to die than to famish\n",
            "\n",
            " _____\n",
            "['first', 'citizen:', 'you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish']\n",
            "all:\n",
            "resolved resolved\n",
            "\n",
            " _____\n",
            "['all:', 'resolved', 'resolved']\n",
            "first citizen:\n",
            "first you know caius marcius is chief enemy to the people\n",
            "\n",
            " _____\n",
            "['first', 'citizen:', 'first', 'you', 'know', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people']\n",
            "all:\n",
            "we know't we know't\n",
            "\n",
            " _____\n",
            "['all:', 'we', \"know't\", 'we', \"know't\"]\n",
            "first citizen:\n",
            "let us kill him and we'll have corn at our own price\n",
            "is't a verdict\n",
            "\n",
            " _____\n",
            "['first', 'citizen:', 'let', 'us', 'kill', 'him', 'and', \"we'll\", 'have', 'corn', 'at', 'our', 'own', 'price', \"is't\", 'a', 'verdict']\n",
            "all:\n",
            "no more talking on't; let it be done: away away\n",
            "\n",
            "second citizen:\n",
            "one word good citizens\n",
            "\n",
            " _____\n",
            "['all:', 'no', 'more', 'talking', \"on't;\", 'let', 'it', 'be', 'done:', 'away', 'away', 'second', 'citizen:', 'one', 'word', 'good', 'citizens']\n",
            "first citizen:\n",
            "we are accounted poor citizens the patricians good\n",
            " _____\n",
            "['first', 'citizen:', 'we', 'are', 'accounted', 'poor', 'citizens', 'the', 'patricians', 'good']\n",
            "what authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity while it were\n",
            "wholesome we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us the object of our misery is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them let us revenge this with\n",
            "our pikes ere we become rakes: for the gods know i\n",
            "speak this in hunger for bread not in thirst for revenge _____\n",
            "['what', 'authority', 'surfeits', 'on', 'would', 'relieve', 'us:', 'if', 'they', 'would', 'yield', 'us', 'but', 'the', 'superfluity', 'while', 'it', 'were', 'wholesome', 'we', 'might', 'guess', 'they', 'relieved', 'us', 'humanely;', 'but', 'they', 'think', 'we', 'are', 'too', 'dear:', 'the', 'leanness', 'that', 'afflicts', 'us', 'the', 'object', 'of', 'our', 'misery', 'is', 'as', 'an', 'inventory', 'to', 'particularise', 'their', 'abundance;', 'our', 'sufferance', 'is', 'a', 'gain', 'to', 'them', 'let', 'us', 'revenge', 'this', 'with', 'our', 'pikes', 'ere', 'we', 'become', 'rakes:', 'for', 'the', 'gods', 'know', 'i', 'speak', 'this', 'in', 'hunger', 'for', 'bread', 'not', 'in', 'thirst', 'for', 'revenge']\n"
          ]
        }
      ],
      "source": [
        "for sentence in text:\n",
        "    print(sentence, \"_____\")\n",
        "    words = sentence.split()\n",
        "    print(words)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Getting the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "AhX8b1ydtrVf"
      },
      "outputs": [],
      "source": [
        "# combine everything into one to make vocabs\n",
        "word_list = list(set(\" \".join(text).split()))\n",
        "word2id = {'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}  #special tokens.\n",
        "\n",
        "#create the word2id\n",
        "for i, w in enumerate(word_list):\n",
        "    word2id[w] = i + 4  #because 0-3 are already occupied\n",
        "    id2word = {i: w for i, w in enumerate(word2id)}\n",
        "    vocab_size = len(word2id)\n",
        "\n",
        "#list of all tokens for whole text\n",
        "token_list = list()\n",
        "for sentence in sentences:\n",
        "    arr = [word2id[word] for sentence in text for word in sentence.split()]\n",
        "    token_list.append(arr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "first\n",
            "citizen:\n",
            "before\n",
            "we\n",
            "proceed\n",
            "any\n",
            "further\n",
            "hear\n",
            "me\n",
            "speak\n",
            "all:\n",
            "speak\n",
            "speak\n",
            "first\n",
            "citizen:\n",
            "you\n",
            "are\n",
            "all\n",
            "resolved\n",
            "rather\n",
            "to\n",
            "die\n",
            "than\n",
            "to\n",
            "famish\n",
            "all:\n",
            "resolved\n",
            "resolved\n",
            "first\n",
            "citizen:\n",
            "first\n",
            "you\n",
            "know\n",
            "caius\n",
            "marcius\n",
            "is\n",
            "chief\n",
            "enemy\n",
            "to\n",
            "the\n",
            "people\n",
            "all:\n",
            "we\n",
            "know't\n",
            "we\n",
            "know't\n",
            "first\n",
            "citizen:\n",
            "let\n",
            "us\n",
            "kill\n",
            "him\n",
            "and\n",
            "we'll\n",
            "have\n",
            "corn\n",
            "at\n",
            "our\n",
            "own\n",
            "price\n",
            "is't\n",
            "a\n",
            "verdict\n",
            "all:\n",
            "no\n",
            "more\n",
            "talking\n",
            "on't;\n",
            "let\n",
            "it\n",
            "be\n",
            "done:\n",
            "away\n",
            "away\n",
            "second\n",
            "citizen:\n",
            "one\n",
            "word\n",
            "good\n",
            "citizens\n",
            "first\n",
            "citizen:\n",
            "we\n",
            "are\n",
            "accounted\n",
            "poor\n",
            "citizens\n",
            "the\n",
            "patricians\n",
            "good\n",
            "what\n",
            "authority\n",
            "surfeits\n",
            "on\n",
            "would\n",
            "relieve\n",
            "us:\n",
            "if\n",
            "they\n",
            "would\n",
            "yield\n",
            "us\n",
            "but\n",
            "the\n",
            "superfluity\n",
            "while\n",
            "it\n",
            "were\n",
            "wholesome\n",
            "we\n",
            "might\n",
            "guess\n",
            "they\n",
            "relieved\n",
            "us\n",
            "humanely;\n",
            "but\n",
            "they\n",
            "think\n",
            "we\n",
            "are\n",
            "too\n",
            "dear:\n",
            "the\n",
            "leanness\n",
            "that\n",
            "afflicts\n",
            "us\n",
            "the\n",
            "object\n",
            "of\n",
            "our\n",
            "misery\n",
            "is\n",
            "as\n",
            "an\n",
            "inventory\n",
            "to\n",
            "particularise\n",
            "their\n",
            "abundance;\n",
            "our\n",
            "sufferance\n",
            "is\n",
            "a\n",
            "gain\n",
            "to\n",
            "them\n",
            "let\n",
            "us\n",
            "revenge\n",
            "this\n",
            "with\n",
            "our\n",
            "pikes\n",
            "ere\n",
            "we\n",
            "become\n",
            "rakes:\n",
            "for\n",
            "the\n",
            "gods\n",
            "know\n",
            "i\n",
            "speak\n",
            "this\n",
            "in\n",
            "hunger\n",
            "for\n",
            "bread\n",
            "not\n",
            "in\n",
            "thirst\n",
            "for\n",
            "revenge\n"
          ]
        }
      ],
      "source": [
        "#testing one sentence\n",
        "for tokens in token_list[0]:\n",
        "    print(id2word[tokens])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "batch_size = 6\n",
        "max_mask   = 5  # max masked tokens when 15% exceed, it will only be max_pred\n",
        "max_len    = 1000 # maximum of length to be padded; "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "TtyOOmRntu8w"
      },
      "outputs": [],
      "source": [
        "def make_batch():\n",
        "    batch = []\n",
        "    positive = negative = 0  #count of batch size;  we want to have half batch that are positive pairs (i.e., next sentence pairs)\n",
        "    while positive != batch_size/2 or negative != batch_size/2:\n",
        "        \n",
        "        #randomly choose two sentence so we can put [SEP]\n",
        "        tokens_a_index, tokens_b_index= randrange(len(sentences)), randrange(len(sentences))\n",
        "        #retrieve the two sentences\n",
        "        tokens_a, tokens_b= token_list[tokens_a_index], token_list[tokens_b_index]\n",
        "\n",
        "        #1. token embedding - append CLS and SEP\n",
        "        input_ids = [word2id['[CLS]']] + tokens_a + [word2id['[SEP]']] + tokens_b + [word2id['[SEP]']]\n",
        "\n",
        "        #2. segment embedding - [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1]\n",
        "        segment_ids = [0] * (1 + len(tokens_a) + 1) + [1] * (len(tokens_b) + 1)\n",
        "\n",
        "        #3. mask language modeling\n",
        "        #masked 15%, but should be at least 1 but does not exceed max_mask\n",
        "        n_pred =  min(max_mask, max(1, int(round(len(input_ids) * 0.15))))\n",
        "        #get the pos that excludes CLS and SEP and shuffle them\n",
        "        cand_maked_pos = [i for i, token in enumerate(input_ids) if token != word2id['[CLS]'] and token != word2id['[SEP]']]\n",
        "        shuffle(cand_maked_pos)\n",
        "        masked_tokens, masked_pos = [], []\n",
        "        #simply loop and change the input_ids to [MASK]\n",
        "        for pos in cand_maked_pos[:n_pred]:\n",
        "            masked_pos.append(pos)  #remember the position\n",
        "            masked_tokens.append(input_ids[pos]) #remember the tokens\n",
        "            #80% replace with a [MASK], but 10% will replace with a random token\n",
        "            if random() < 0.1:  # 10%\n",
        "                index = randint(0, vocab_size - 1) # random index in vocabulary\n",
        "                input_ids[pos] = word2id[id2word[index]] # replace\n",
        "            elif random() < 0.9:  # 80%\n",
        "                input_ids[pos] = word2id['[MASK]'] # make mask\n",
        "            else:  #10% do nothing\n",
        "                pass\n",
        "\n",
        "        # pad the input_ids and segment ids until the max len\n",
        "        n_pad = max_len - len(input_ids)\n",
        "        input_ids.extend([0] * n_pad)\n",
        "        segment_ids.extend([0] * n_pad)\n",
        "\n",
        "        # pad the masked_tokens and masked_pos to make sure the lenth is max_mask\n",
        "        if max_mask > n_pred:\n",
        "            n_pad = max_mask - n_pred\n",
        "            masked_tokens.extend([0] * n_pad)\n",
        "            masked_pos.extend([0] * n_pad)\n",
        "\n",
        "        #check if first sentence is really comes before the second sentence\n",
        "        #also make sure positive is exactly half the batch size\n",
        "        if tokens_a_index + 1 == tokens_b_index and positive < batch_size / 2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, True]) # IsNext\n",
        "            positive += 1\n",
        "        elif tokens_a_index + 1 != tokens_b_index and negative < batch_size/2:\n",
        "            batch.append([input_ids, segment_ids, masked_tokens, masked_pos, False]) # NotNext\n",
        "            negative += 1\n",
        "            \n",
        "    return batch\n",
        "\n",
        "batch = make_batch()\n",
        "\n",
        "len(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Deconstruct using map and zip\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "input_ids.shape, segment_ids.shape, masked_tokens.shape, masked_pos.shape, isNext.shape\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = input_ids.to(device), segment_ids.to(device), masked_tokens.to(device), masked_pos.to(device), isNext.to(device)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT Model definition"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Embedding(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.tok_embed = nn.Embedding(vocab_size, d_model)  # token embedding\n",
        "        self.pos_embed = nn.Embedding(max_len, d_model)      # position embedding\n",
        "        self.seg_embed = nn.Embedding(n_segments, d_model)  # segment(token type) embedding\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(self, x, seg):\n",
        "        #x, seg: (bs, len)\n",
        "        seq_len = x.size(1)\n",
        "        pos = torch.arange(seq_len, dtype=torch.long)\n",
        "        pos = pos.unsqueeze(0).expand_as(x)  # (len,) -> (bs, len)\n",
        "        embedding = self.tok_embed(x) + self.pos_embed(pos) + self.seg_embed(seg)\n",
        "        return self.norm(embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Attention mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "s1PGksqBNuZM"
      },
      "outputs": [],
      "source": [
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "    batch_size, len_q = seq_q.size()\n",
        "    batch_size, len_k = seq_k.size()\n",
        "    # eq(zero) is PAD token\n",
        "    pad_attn_mask = seq_k.data.eq(0).unsqueeze(1)  # batch_size x 1 x len_k(=len_q), one is masking\n",
        "    return pad_attn_mask.expand(batch_size, len_q, len_k)  # batch_size x len_q x len_k"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.enc_self_attn = MultiHeadAttention()\n",
        "        self.pos_ffn       = PoswiseFeedForwardNet()\n",
        "\n",
        "    def forward(self, enc_inputs, enc_self_attn_mask):\n",
        "        enc_outputs, attn = self.enc_self_attn(enc_inputs, enc_inputs, enc_inputs, enc_self_attn_mask) # enc_inputs to same Q,K,V\n",
        "        enc_outputs = self.pos_ffn(enc_outputs) # enc_outputs: [batch_size x len_q x d_model]\n",
        "        return enc_outputs, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {},
      "outputs": [],
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ScaledDotProductAttention, self).__init__()\n",
        "\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        scores = torch.matmul(Q, K.transpose(-1, -2)) / np.sqrt(d_k) # scores : [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        scores.masked_fill_(attn_mask, -1e9) # Fills elements of self tensor with value where mask is one.\n",
        "        attn = nn.Softmax(dim=-1)(scores)\n",
        "        context = torch.matmul(attn, V)\n",
        "        return context, attn "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_layers = 6    # number of Encoder of Encoder Layer\n",
        "n_heads  = 8    # number of heads in Multi-Head Attention\n",
        "d_model  = 768  # Embedding Size\n",
        "d_ff = 768 * 4  # 4*d_model, FeedForward dimension\n",
        "d_k = d_v = 64  # dimension of K(=Q), V\n",
        "n_segments = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {},
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.W_Q = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_K = nn.Linear(d_model, d_k * n_heads)\n",
        "        self.W_V = nn.Linear(d_model, d_v * n_heads)\n",
        "    def forward(self, Q, K, V, attn_mask):\n",
        "        # q: [batch_size x len_q x d_model], k: [batch_size x len_k x d_model], v: [batch_size x len_k x d_model]\n",
        "        residual, batch_size = Q, Q.size(0)\n",
        "        # (B, S, D) -proj-> (B, S, D) -split-> (B, S, H, W) -trans-> (B, H, S, W)\n",
        "        q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "        k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k]\n",
        "        v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "        attn_mask = attn_mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "\n",
        "        # context: [batch_size x n_heads x len_q x d_v], attn: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "        context, attn = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
        "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) # context: [batch_size x len_q x n_heads * d_v]\n",
        "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
        "        return nn.LayerNorm(d_model)(output + residual), attn # output: [batch_size x len_q x d_model]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {},
      "outputs": [],
      "source": [
        "class PoswiseFeedForwardNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PoswiseFeedForwardNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch_size, len_seq, d_model) -> (batch_size, len_seq, d_ff) -> (batch_size, len_seq, d_model)\n",
        "        return self.fc2(F.gelu(self.fc1(x)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "OZ0TJ84W4SZw"
      },
      "outputs": [],
      "source": [
        "class BERT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BERT, self).__init__()\n",
        "        self.embedding = Embedding()\n",
        "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])\n",
        "        self.fc = nn.Linear(d_model, d_model)\n",
        "        self.activ = nn.Tanh()\n",
        "        self.linear = nn.Linear(d_model, d_model)\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "        self.classifier = nn.Linear(d_model, 2)\n",
        "        # decoder is shared with embedding layer\n",
        "        embed_weight = self.embedding.tok_embed.weight\n",
        "        n_vocab, n_dim = embed_weight.size()\n",
        "        self.decoder = nn.Linear(n_dim, n_vocab, bias=False)\n",
        "        self.decoder.weight = embed_weight\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(n_vocab))\n",
        "\n",
        "    def forward(self, input_ids, segment_ids, masked_pos):\n",
        "        output = self.embedding(input_ids, segment_ids)\n",
        "        enc_self_attn_mask = get_attn_pad_mask(input_ids, input_ids)\n",
        "        for layer in self.layers:\n",
        "            output, enc_self_attn = layer(output, enc_self_attn_mask)\n",
        "        # output : [batch_size, len, d_model], attn : [batch_size, n_heads, d_mode, d_model]\n",
        "        \n",
        "        # 1. predict next sentence\n",
        "        # it will be decided by first token(CLS)\n",
        "        h_pooled   = self.activ(self.fc(output[:, 0])) # [batch_size, d_model]\n",
        "        logits_nsp = self.classifier(h_pooled) # [batch_size, 2]\n",
        "\n",
        "        # 2. predict the masked token\n",
        "        masked_pos = masked_pos[:, :, None].expand(-1, -1, output.size(-1)) # [batch_size, max_pred, d_model]\n",
        "        h_masked = torch.gather(output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
        "        h_masked  = self.norm(F.gelu(self.linear(h_masked)))\n",
        "        logits_lm = self.decoder(h_masked) + self.decoder_bias # [batch_size, max_pred, n_vocab]\n",
        "\n",
        "        return logits_lm, logits_nsp\n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8UAG3SEP4UbU",
        "outputId": "bc6f202f-df37-4fac-843c-fb86bdb777b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 00 loss = 77.911346\n",
            "Epoch: 10 loss = 92.424904\n",
            "Epoch: 20 loss = 76.264832\n",
            "Epoch: 30 loss = 42.310841\n",
            "Epoch: 40 loss = 23.345474\n",
            "Epoch: 50 loss = 19.225452\n",
            "Epoch: 60 loss = 17.568678\n",
            "Epoch: 70 loss = 10.960737\n",
            "Epoch: 80 loss = 8.401817\n",
            "Epoch: 90 loss = 6.422518\n"
          ]
        }
      ],
      "source": [
        "num_epoch = 100\n",
        "model = BERT()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "batch = make_batch()\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(*batch))\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = input_ids.to(device), segment_ids.to(device), masked_tokens.to(device), masked_pos.to(device), isNext.to(device)\n",
        "for epoch in range(num_epoch):\n",
        "    optimizer.zero_grad()\n",
        "    logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)    \n",
        "\n",
        "    loss_lm = criterion(logits_lm.transpose(1, 2), masked_tokens) # for masked LM\n",
        "    loss_lm = (loss_lm.float()).mean()\n",
        "\n",
        "    loss_nsp = criterion(logits_nsp, isNext) # for sentence classification\n",
        "    \n",
        "    loss = loss_lm + loss_nsp\n",
        "    if epoch % 10 == 0:\n",
        "        print('Epoch:', '%02d' % (epoch), 'loss =', '{:.6f}'.format(loss))\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uD3K8T6B4YJp",
        "outputId": "e057405b-1f78-431c-fa71-032a738fb848"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['[CLS]', 'first', 'citizen:', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', 'all:', 'speak', 'speak', 'first', 'citizen:', 'you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish', 'all:', 'resolved', 'resolved', 'first', 'citizen:', 'first', 'you', '[MASK]', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people', 'all:', 'we', \"know't\", 'we', \"know't\", 'first', 'citizen:', 'let', 'us', 'kill', 'him', 'and', \"we'll\", 'have', 'corn', 'at', 'our', 'own', 'price', \"is't\", 'a', 'verdict', 'all:', 'no', 'more', 'talking', \"on't;\", 'let', 'it', 'be', 'done:', 'away', 'away', 'second', 'citizen:', 'one', 'word', 'good', 'citizens', 'first', 'citizen:', 'we', 'are', 'accounted', 'poor', 'citizens', 'the', 'patricians', 'good', 'what', 'authority', 'surfeits', 'on', 'would', 'relieve', 'us:', 'if', 'they', 'would', 'yield', 'us', 'but', 'the', 'superfluity', 'while', 'it', 'were', 'wholesome', 'we', 'might', 'guess', 'they', 'relieved', 'us', 'humanely;', 'but', 'they', 'think', 'we', 'are', 'too', 'dear:', 'the', 'leanness', 'that', 'afflicts', 'us', 'the', 'object', 'of', 'our', 'misery', 'is', 'as', '[MASK]', 'inventory', 'to', 'particularise', 'their', 'abundance;', 'our', 'sufferance', 'is', 'a', 'gain', 'to', 'them', 'let', 'us', 'revenge', 'this', 'with', 'our', 'pikes', 'ere', 'we', 'become', 'rakes:', 'for', 'the', 'gods', 'know', 'i', 'speak', 'this', 'in', 'hunger', 'for', 'bread', 'not', 'in', 'thirst', 'for', 'revenge', '[SEP]', 'first', 'citizen:', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', 'all:', 'speak', 'speak', 'first', 'citizen:', 'you', 'are', 'all', 'resolved', 'rather', 'to', 'die', 'than', 'to', 'famish', 'all:', 'resolved', 'resolved', '[MASK]', 'citizen:', 'first', 'you', 'know', 'caius', 'marcius', 'is', 'chief', 'enemy', 'to', 'the', 'people', 'all:', 'we', \"know't\", 'we', \"know't\", 'first', 'citizen:', 'let', 'us', 'kill', 'him', 'and', \"we'll\", 'have', 'corn', 'at', 'our', 'own', 'price', \"is't\", 'a', 'verdict', 'all:', 'no', 'more', 'talking', \"on't;\", 'let', 'it', 'be', 'done:', 'away', 'away', 'second', 'citizen:', 'one', 'word', 'good', 'citizens', 'first', 'citizen:', 'we', 'are', 'accounted', 'poor', 'citizens', 'the', 'patricians', 'good', 'what', 'authority', 'surfeits', 'on', 'would', 'relieve', 'us:', 'if', 'they', 'would', 'yield', 'us', 'but', 'the', 'superfluity', 'while', 'it', 'were', 'wholesome', 'we', 'might', 'guess', 'they', 'relieved', 'us', 'humanely;', 'but', 'they', 'think', 'we', 'are', 'too', 'dear:', '[MASK]', 'leanness', 'that', 'afflicts', 'us', 'the', 'object', 'of', 'our', 'misery', 'is', 'as', 'an', 'inventory', 'to', 'particularise', 'their', 'abundance;', 'our', 'sufferance', 'is', 'a', 'gain', 'to', 'them', 'let', 'us', 'revenge', 'this', 'with', 'our', 'pikes', 'ere', 'we', 'become', 'rakes:', 'for', 'the', 'gods', 'know', 'i', 'speak', 'this', 'in', 'hunger', 'for', 'bread', 'not', 'in', 'thirst', '[MASK]', 'revenge', '[SEP]']\n",
            "masked tokens (words) :  ['for', 'know', 'an', 'the', 'first']\n",
            "masked tokens list :  [68, 61, 14, 5, 24]\n",
            "masked tokens (words) :  ['for', 'for', 'for', 'for', 'for']\n",
            "predict masked tokens list :  [tensor(68), tensor(68), tensor(68), tensor(68), tensor(68)]\n",
            "tensor(1)\n",
            "isNext :  False\n",
            "predict isNext :  True\n"
          ]
        }
      ],
      "source": [
        "# Predict mask tokens ans isNext\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = map(torch.LongTensor, zip(batch[2]))\n",
        "print([id2word[w.item()] for w in input_ids[0] if id2word[w.item()] != '[PAD]'])\n",
        "input_ids, segment_ids, masked_tokens, masked_pos, isNext = input_ids.to(device), segment_ids.to(device), masked_tokens.to(device), masked_pos.to(device), isNext.to(device)\n",
        "\n",
        "logits_lm, logits_nsp = model(input_ids, segment_ids, masked_pos)\n",
        "\n",
        "logits_lm = logits_lm.data.max(2)[1][0].data\n",
        "\n",
        "print('masked tokens (words) : ',[id2word[pos.item()] for pos in masked_tokens[0]])\n",
        "print('masked tokens list : ',[pos.item() for pos in masked_tokens[0]])\n",
        "print('masked tokens (words) : ',[id2word[pos.item()] for pos in logits_lm])\n",
        "print('predict masked tokens list : ', [pos for pos in logits_lm])\n",
        "\n",
        "logits_nsp = logits_nsp.data.max(1)[1][0].data\n",
        "print(logits_nsp)\n",
        "print('isNext : ', True if isNext else False)\n",
        "print('predict isNext : ',True if logits_nsp else False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This notebook is only for demonstration of pre-training BERT. The actual BERT model is much larger and also uses lot of training data: Toronto BookCorpus (800M words) and English Wikipedia (2,500M words). Training this requires sufficient compute and not possible on a single local GPU machine.  \n",
        "\n",
        "### After the model is pre-trained, it can be fine-tuned for downstream tasks such as sequence classficiation, Natural language inference etc. "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
